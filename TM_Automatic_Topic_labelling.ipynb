{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Automatic Topic labelling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMb/eDkjDnCB0tU2hgLnzPj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bharatbajoria/Summer-Internship/blob/master/Automatic_Topic_labelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SXOAX7NXKj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def automatic_labels(idealtopics,wikipedia,RegexpTokenizer,model1,stopwords,repetitions):\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  en_stop = set(stopwords.words('english'))\n",
        "  en_stop.add('the')#The is not in stopwords\n",
        "  \n",
        "  dup_model=model1\n",
        "  \n",
        "  topic_count=1\n",
        "  for reruns in range(repetitions):\n",
        "    print(\"Repetition:-\",reruns+1)\n",
        "    Automatic_labels=[]\n",
        "    for x in idealtopics:\n",
        "\n",
        "      print(\"Finding label for Topic:\",topic_count)\n",
        "      topic_count+=1\n",
        "      \n",
        "      contents=[]\n",
        "      title_vector=[]   \n",
        "      index_topic=[]\n",
        "      #ndex_topic[0][1] # index_topic[i] is a list of titles obtained for word-1\n",
        "      ## index_topic[i][j] is a list of words in j-th title obtained for word-1\n",
        "      word_limit=20\n",
        "      x=x[:word_limit]\n",
        "      count=0\n",
        "      k=0\n",
        "      for j in x:\n",
        "        contents=[]\n",
        "        index_word=[]\n",
        "        file_title=wikipedia.search(j,results=10,suggestion= True)\n",
        "        print(\"\\n\\nTitle for word :\", j)\n",
        "        print(file_title[0])\n",
        "        print(count)\n",
        "        \n",
        "        for l in range(len(file_title[0])):\n",
        "          continue_var=0\n",
        "          try:\n",
        "            y = wikipedia.search(file_title[0][l])[2]\n",
        "            y = wikipedia.page(y)\n",
        "        # except wikipedia.DisambiguationError as e:\n",
        "          #  s = e.options[0] \n",
        "          # y = wikipedia.page(s)\n",
        "          except :             \n",
        "            continue_var=1            \n",
        "\n",
        "          if (continue_var==0 or type(y)=='str'):       \n",
        "            token=tokenizer.tokenize(y.content)\n",
        "            token=[i for i in token if(not(str(i).isdigit() or not(str(i).isalpha())) and len(str(i)) > 2 )]\n",
        "            token=[i.lower() for i in token if( i not in en_stop)]\n",
        "            contents.append(token)\n",
        "          count+=1\n",
        "\n",
        "        \n",
        "          \n",
        "        dup_model.build_vocab(contents,update= True)#content is a 2-d list\n",
        "        dup_model.train(contents, total_examples=len(contents),epochs=2)\n",
        "        for i in range(len(file_title[0])):\n",
        "          \n",
        "          \n",
        "          y= file_title[0][i].lower()\n",
        "          z=y.split()\n",
        "          for m in range(len(z)):\n",
        "            index=[]\n",
        "            p=list(z[m])\n",
        "            for k in range(len(p)):\n",
        "              if (p[k]==\"-\" or p[k]==\",\"):\n",
        "                p[k]=\" \"\n",
        "\n",
        "              if not(p[k].isalpha()):\n",
        "                index.append(k)\n",
        "            for k in range(len(index)):\n",
        "              p.pop(index[len(index)-k-1])\n",
        "            \n",
        "            k=''\n",
        "            for w in p:\n",
        "              k+=w\n",
        "            z[m]=k\n",
        "\n",
        "          z=[k for k in z if not(k in en_stop or k== j or k==\"\" or k==\",\")]\n",
        "          \n",
        "          index_word.append(z)\n",
        "          dup_model.build_vocab(index_word,update= True)\n",
        "          dup_model.train(index_word, total_examples=len(contents),epochs=2)\n",
        "        \n",
        "          \n",
        "        index_topic.append(index_word)\n",
        "\n",
        "      max_score=0\n",
        "      max_list=[]\n",
        "      for i in range(len(index_topic)):\n",
        "        o=0\n",
        "        for j in range(len(index_topic[i])):\n",
        "          o=0\n",
        "          for k in range(len(index_topic[i][j])):\n",
        "            for p in range(len(x)):\n",
        "              o+=dup_model.similarity(index_topic[i][j][k],x[p])\n",
        "          if len(index_topic[i][j])>0:\n",
        "            o= o/len(index_topic[i][j])\n",
        "          if o>max_score:\n",
        "            max_score=o\n",
        "            max_list=[i,j]\n",
        "\n",
        "      file_title=wikipedia.search(x[max_list[0]],results=10,suggestion= False)\n",
        "      Automatic_labels.append(file_title[max_list[1]])\n",
        "\n",
        "  return Automatic_labels"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
